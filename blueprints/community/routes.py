from flask import Blueprint, current_app as app, request, session, jsonify
import logging # Add logging
import os
import re
# import requests # Already imported below if needed
import openai
import requests
from datetime import datetime, timedelta # Import timedelta
from collections import Counter
from pymongo import MongoClient
from dotenv import load_dotenv
from bson import ObjectId  # Import to handle ObjectId serialization
from blueprints.utils.locale import get_country_language # Import the function

# Removed redundant imports and ensured proper error handling

community_bp = Blueprint("community", __name__)
logger = logging.getLogger(__name__) # Add logger instance
# Ensure logging is configured (ideally in app.py)
# logging.basicConfig(level=logging.DEBUG)

# ðŸŒ Mongo Setup
load_dotenv()
MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017/")
client = MongoClient(MONGO_URI)
db = client["pihappy"]
users_collection = db["users"] # Access users collection
community_input_collection = db["community_input"]
community_analysis_collection = db["community_analysis"] # Collection for storing results

# ðŸ› ï¸ Mongo replacements
def laad_community_input():
    try:
        # Convert ObjectId to string for JSON serialization
        return [
            {**doc, "_id": str(doc["_id"])} for doc in community_input_collection.find()
        ]
    except Exception as e:
        app.logger.error(f"Error loading community input: {e}")
        return []

# def sla_community_input_op(data): # This function seems unused, commenting out
#     community_input_collection.delete_many({})
#     community_input_collection.insert_many(data)


@community_bp.route('/landen', methods=['GET'])
def haal_landen_op():
    try:
        landen_cursor = db.countries.find({}, {"_id": 0})  # Gebruik collectie 'countries'
        landen_lijst = [
            {
                "naam": land["name"],
                "code": land["code"],
                "vlag": land.get("flag", ""),
                "taal": land.get("language", "Unknown")
            }
            for land in landen_cursor
        ]
        landen_lijst.sort(key=lambda x: x["naam"])  # Sorteer op naam
        return jsonify({"landen": landen_lijst})
    except Exception as e:
        return jsonify({"error": str(e)}), 500



@community_bp.route("/community_input", methods=["GET"])
def get_community_input():
    geschiedenis = laad_community_input()
    return jsonify({"status": "success", "data": geschiedenis})

@community_bp.route("/community_input/send", methods=["POST"])
def send_community_input():
    logger.debug("Received request for /community_input/send")
    if "gebruiker" not in session:
        logger.warning("Unauthorized attempt to send community input. No 'gebruiker' in session.")
        return jsonify({"status": "error", "message": "Authentication required."}), 401

    logger.debug(f"Request JSON data: {request.json}")
    input_text = request.json.get("input", "").strip()
    gebruiker_info = session.get("gebruiker", {})
    gebruiker_naam = gebruiker_info.get("naam", "Unknown User") # Safely get name
    user_id = gebruiker_info.get("id") # Get user ID
    if not user_id:
        logger.error("User ID missing from session during community input send.")
        return jsonify({"status": "error", "message": "User session error."}), 500
    logger.debug(f"Input text: '{input_text}', User name: '{gebruiker_naam}'")

    if not input_text:
        logger.warning("Received empty input text.")
        return jsonify({"status": "error", "message": "Your input is empty."}) # Changed to English

    # Check input length
    if len(input_text) > 250:
        logger.warning(f"Received input exceeding 250 characters from user {gebruiker_naam} (ID: {user_id}).")
        return jsonify({"status": "error", "message": "Your input is too long (max 250 characters)."}), 400

    # --- Cooldown Check ---
    try:
        user_object_id = ObjectId(user_id)
        user_doc = users_collection.find_one({"_id": user_object_id})
        if user_doc and "last_community_input_time" in user_doc:
            last_post_time = user_doc["last_community_input_time"]
            # Ensure last_post_time is offset-aware for comparison if needed, or use naive consistently
            # Assuming stored time is naive UTC from datetime.utcnow()
            time_since_last_post = datetime.utcnow() - last_post_time
            if time_since_last_post < timedelta(hours=1):
                wait_time = timedelta(hours=1) - time_since_last_post
                minutes_left = int(wait_time.total_seconds() // 60) + 1 # Round up minutes
                logger.warning(f"User {user_id} tried to post community input too soon. Wait time: {minutes_left} minutes.")
                return jsonify({
                    "status": "error",
                    "message": f"You can post again in {minutes_left} minute(s)."
                }), 429 # Too Many Requests
    except Exception as e:
        logger.error(f"Error checking cooldown for user {user_id}: {e}", exc_info=True)
        return jsonify({"status": "error", "message": "Error checking post cooldown."}), 500
    # --- End Cooldown Check ---

    nieuwe_input = {
        "gebruiker": gebruiker_naam,
        "userId": user_object_id,
        "country": gebruiker_info.get("land", "Unknown"), # Store user's country
        "input": input_text,
        "tijd": datetime.utcnow()
    }

    logger.debug(f"Attempting to insert into community_input_collection: {nieuwe_input}")
    try:
        result = community_input_collection.insert_one(nieuwe_input)
        if result.inserted_id:
            logger.info(f"Successfully inserted community input with ID: {result.inserted_id}")

            # Podcast generation logic moved to /analyse route

            # --- Update User's Last Post Time ---
            try:
                users_collection.update_one(
                    {"_id": user_object_id},
                    {"$set": {"last_community_input_time": nieuwe_input["tijd"]}}
                )
                logger.info(f"Updated last_community_input_time for user {user_id}.")
            except Exception as update_err:
                # Log the error but don't fail the request, as the input was saved
                logger.error(f"Failed to update last_community_input_time for user {user_id}: {update_err}", exc_info=True)
            # --- End Update User Time ---

            # --- Trigger Analysis Check ---
            try:
                input_count = community_input_collection.count_documents({})
                logger.info(f"Total community inputs after insert: {input_count}")
                if input_count > 0 and input_count % 2 == 0:
                    logger.info(f"Input count ({input_count}) is even, triggering analysis.")
                    # In a production app, this should be asynchronous (e.g., Celery task)
                    # For simplicity here, we call it synchronously.
                    perform_community_analysis(session.get("gebruiker", {}).get("land", "other"))
            except Exception as analysis_trigger_err:
                 logger.error(f"Error during analysis trigger check: {analysis_trigger_err}", exc_info=True)
            # --- End Trigger Analysis Check ---

            return jsonify({"status": "success", "message": "Input submitted!"})
        else:
            logger.error("community_input_collection.insert_one completed but inserted_id is missing.")
            return jsonify({"status": "error", "message": "Database error on save (no ID)."}), 500
    except Exception as e:
        logger.error(f"Error inserting community input into MongoDB: {e}", exc_info=True)
        return jsonify({"status": "error", "message": "Database error on save."}), 500

# --- Helper Function for Analysis ---
def perform_community_analysis(user_country_code="other"):
    """Fetches inputs, analyzes them via OpenAI, generates podcast, and stores the result."""
    logger.info("Starting perform_community_analysis...")
    taal_code = get_country_language(user_country_code)
    logger.debug(f"Analysis Language Code: {taal_code}")

    # Language-specific prompts (copied from original /analyse route)
    ANALYSIS_SYSTEM_MSG = { "en": "You are an AI that analyzes and summarizes community feedback.", "nl": "Jij bent een AI die community feedback analyseert en samenvat.", "es": "Eres una IA que analiza y resume los comentarios de la comunidad.", "de": "Du bist eine KI, die Community-Feedback analysiert und zusammenfasst.", "fr": "Tu es une IA qui analyse et rÃ©sume les retours de la communautÃ©.", "zh": "ä½ æ˜¯ä¸€ä¸ªåˆ†æžå’Œæ€»ç»“ç¤¾åŒºåé¦ˆçš„äººå·¥æ™ºèƒ½ã€‚", "hi": "à¤†à¤ª à¤à¤• AI à¤¹à¥ˆà¤‚ à¤œà¥‹ à¤¸à¤¾à¤®à¥à¤¦à¤¾à¤¯à¤¿à¤• à¤ªà¥à¤°à¤¤à¤¿à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤•à¤¾ à¤µà¤¿à¤¶à¥à¤²à¥‡à¤·à¤£ à¤”à¤° à¤¸à¤¾à¤°à¤¾à¤‚à¤¶ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤","id": "Anda adalah AI yang menganalisis dan merangkum umpan balik komunitas.","ur": "Ø¢Ù¾ Ø§ÛŒÚ© AI ÛÛŒÚº Ø¬Ùˆ Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ú©Û’ ØªØ§Ø«Ø±Ø§Øª Ú©Ø§ ØªØ¬Ø²ÛŒÛ Ø§ÙˆØ± Ø®Ù„Ø§ØµÛ Ú©Ø±ØªØ§ ÛÛ’Û”","pt": "VocÃª Ã© uma IA que analisa e resume o feedback da comunidade.","bn": "à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ AI à¦¯à¦¿à¦¨à¦¿ à¦¸à¦®à§à¦ªà§à¦°à¦¦à¦¾à¦¯à¦¼à§‡à¦° à¦ªà§à¦°à¦¤à¦¿à¦•à§à¦°à¦¿à¦¯à¦¼à¦¾ à¦¬à¦¿à¦¶à§à¦²à§‡à¦·à¦£ à¦à¦¬à¦‚ à¦¸à¦‚à¦•à§à¦·à¦¿à¦ªà§à¦¤à¦¸à¦¾à¦° à¦•à¦°à§‡à¦¨à¥¤","ru": "Ð’Ñ‹ â€” Ð˜Ð˜, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¸ Ð¾Ð±Ð¾Ð±Ñ‰Ð°ÐµÑ‚ Ð¾Ñ‚Ð·Ñ‹Ð²Ñ‹ ÑÐ¾Ð¾Ð±Ñ‰ÐµÑÑ‚Ð²Ð°.","ja": "ã‚ãªãŸã¯ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åˆ†æžã—è¦ç´„ã™ã‚‹AIã§ã™ã€‚","tl": "Ikaw ay isang AI na nagsusuri at nagbubuod ng feedback ng komunidad.","vi": "Báº¡n lÃ  má»™t AI phÃ¢n tÃ­ch vÃ  tÃ³m táº¯t pháº£n há»“i cá»§a cá»™ng Ä‘á»“ng.","am": "áŠ¥áˆ­áˆµá‹Ž á‹¨áˆ›áˆ…á‰ áˆ¨áˆ°á‰¥ áŒá‰¥áˆ¨áˆ˜áˆáˆµáŠ• á‹¨áˆšá‰°áŠá‰µáŠ‘ áŠ¥áŠ“ á‹¨áˆšá‹«áŒ á‰ƒáˆáˆ‰ AI áŠá‹Žá‰µá¢","ar": "Ø£Ù†Øª Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ­Ù„Ù„ ÙˆÙŠÙ„Ø®Øµ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ù…Ø¬ØªÙ…Ø¹.","fa": "Ø´Ù…Ø§ ÛŒÚ© Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯ Ø¬Ø§Ù…Ø¹Ù‡ Ø±Ø§ ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ùˆ Ø®Ù„Ø§ØµÙ‡ Ù…ÛŒ Ú©Ù†Ø¯.","tr": "Topluluk geri bildirimlerini analiz eden ve Ã¶zetleyen bir yapay zekasÄ±nÄ±z.","ko": "ì»¤ë®¤ë‹ˆí‹° í”¼ë“œë°±ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ëŠ” AIìž…ë‹ˆë‹¤.","th": "à¸„à¸¸à¸“à¸„à¸·à¸­ AI à¸—à¸µà¹ˆà¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹à¸¥à¸°à¸ªà¸£à¸¸à¸›à¸„à¸§à¸²à¸¡à¸„à¸´à¸”à¹€à¸«à¹‡à¸™à¸‚à¸­à¸‡à¸Šà¸¸à¸¡à¸Šà¸™",}
    ANALYSIS_USER_PROMPT = { "en": "Analyze the following input provided by the community and provide a summary of the main themes and opinions. Respond in English.", "nl": "Analyseer deze input van de community en geef een samenvatting van de belangrijkste thema's en meningen. Antwoord in het Nederlands.", "es": "Analiza los siguientes comentarios proporcionados por la comunidad y proporciona un resumen de los temas y opiniones principales. Responde en espaÃ±ol.", "de": "Analysiere die folgenden Eingaben der Community und erstelle eine Zusammenfassung der Hauptthemen und Meinungen. Antworte auf Deutsch.", "fr": "Analysez les commentaires suivants fournis par la communautÃ© et fournissez un rÃ©sumÃ© des principaux thÃ¨mes et opinions. RÃ©pondez en franÃ§ais.", "zh": "åˆ†æžç¤¾åŒºæä¾›çš„ä»¥ä¸‹åé¦ˆï¼Œå¹¶æä¾›ä¸»è¦ä¸»é¢˜å’Œæ„è§çš„æ‘˜è¦ã€‚è¯·ç”¨ä¸­æ–‡å›žç­”ã€‚", "hi": "à¤¸à¤®à¥à¤¦à¤¾à¤¯ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤¿à¤ à¤—à¤ à¤¨à¤¿à¤®à¥à¤¨à¤²à¤¿à¤–à¤¿à¤¤ à¤‡à¤¨à¤ªà¥à¤Ÿ à¤•à¤¾ à¤µà¤¿à¤¶à¥à¤²à¥‡à¤·à¤£ à¤•à¤°à¥‡à¤‚ à¤”à¤° à¤®à¥à¤–à¥à¤¯ à¤µà¤¿à¤·à¤¯à¥‹à¤‚ à¤”à¤° à¤µà¤¿à¤šà¤¾à¤°à¥‹à¤‚ à¤•à¤¾ à¤¸à¤¾à¤°à¤¾à¤‚à¤¶ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¥‡à¤‚à¥¤ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤‚à¥¤","id": "Analisis masukan berikut yang diberikan oleh komunitas dan berikan ringkasan tema dan opini utama. Tanggapi dalam Bahasa Indonesia.","ur": "Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ú©ÛŒ Ø·Ø±Ù Ø³Û’ ÙØ±Ø§ÛÙ… Ú©Ø±Ø¯Û Ø¯Ø±Ø¬ Ø°ÛŒÙ„ Ø§Ù† Ù¾Ù¹ Ú©Ø§ ØªØ¬Ø²ÛŒÛ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§ÛÙ… Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§ÙˆØ± Ø¢Ø±Ø§Ø¡ Ú©Ø§ Ø®Ù„Ø§ØµÛ ÙØ±Ø§ÛÙ… Ú©Ø±ÛŒÚºÛ” Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚºÛ”","pt": "Analise a entrada a seguir fornecida pela comunidade e forneÃ§a um resumo dos principais temas e opiniÃµes. Responda em portuguÃªs.","bn": "à¦¸à¦®à§à¦ªà§à¦°à¦¦à¦¾à¦¯à¦¼ à¦¦à§à¦¬à¦¾à¦°à¦¾ à¦ªà§à¦°à¦¦à¦¤à§à¦¤ à¦¨à¦¿à¦®à§à¦¨à¦²à¦¿à¦–à¦¿à¦¤ à¦‡à¦¨à¦ªà§à¦Ÿ à¦¬à¦¿à¦¶à§à¦²à§‡à¦·à¦£ à¦•à¦°à§à¦¨ à¦à¦¬à¦‚ à¦ªà§à¦°à¦§à¦¾à¦¨ à¦¥à¦¿à¦® à¦à¦¬à¦‚ à¦®à¦¤à¦¾à¦®à¦¤à§‡à¦° à¦à¦•à¦Ÿà¦¿ à¦¸à¦¾à¦°à¦¸à¦‚à¦•à§à¦·à§‡à¦ª à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§à¦¨à¥¤ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¿à¦¨à¥¤","ru": "ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ, Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÑÑ‚Ð²Ð¾Ð¼, Ð¸ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²ÑŒÑ‚Ðµ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ð¸Ð·Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ñ… Ñ‚ÐµÐ¼ Ð¸ Ð¼Ð½ÐµÐ½Ð¸Ð¹. ÐžÑ‚Ð²ÐµÑ‚ÑŒÑ‚Ðµ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ.","ja": "ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‹ã‚‰æä¾›ã•ã‚ŒãŸä»¥ä¸‹ã®å…¥åŠ›ã‚’åˆ†æžã—ã€ä¸»è¦ãªãƒ†ãƒ¼ãƒžã¨æ„è¦‹ã®è¦ç´„ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªžã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚","tl": "Suriin ang sumusunod na input na ibinigay ng komunidad at magbigay ng buod ng mga pangunahing tema at opinyon. Tumugon sa Tagalog.","vi": "PhÃ¢n tÃ­ch Ä‘áº§u vÃ o sau Ä‘Ã¢y do cá»™ng Ä‘á»“ng cung cáº¥p vÃ  cung cáº¥p báº£n tÃ³m táº¯t cÃ¡c chá»§ Ä‘á» vÃ  Ã½ kiáº¿n chÃ­nh. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.","am": "á‰ áˆ›áˆ…á‰ áˆ¨áˆ°á‰¡ á‹¨á‰€áˆ¨á‰ á‹áŠ• á‹¨áˆšáŠ¨á‰°áˆˆá‹áŠ• áŒá‰¥áŠ á‰µ á‹­á‰°áŠ•á‰µáŠ‘ áŠ¥áŠ“ á‹¨á‹‹áŠ“ á‹‹áŠ“ áŒ­á‰¥áŒ¦á‰½áŠ• áŠ¥áŠ“ áŠ áˆµá‰°á‹«á‹¨á‰¶á‰½áŠ• áˆ›áŒ á‰ƒáˆˆá‹« á‹«á‰…áˆ­á‰¡á¢ á‰ áŠ áˆ›áˆ­áŠ› áˆáˆ‹áˆ½ á‹­áˆµáŒ¡á¢","ar": "Ø­Ù„Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ ÙˆÙ‚Ø¯Ù… Ù…Ù„Ø®ØµÙ‹Ø§ Ù„Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ ÙˆØ§Ù„Ø¢Ø±Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.","fa": "ÙˆØ±ÙˆØ¯ÛŒ Ø²ÛŒØ± Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø¬Ø§Ù…Ø¹Ù‡ Ø±Ø§ ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯ Ùˆ Ø®Ù„Ø§ØµÙ‡ Ø§ÛŒ Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ùˆ Ù†Ø¸Ø±Ø§Øª Ø§ØµÙ„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯.","tr": "Topluluk tarafÄ±ndan saÄŸlanan aÅŸaÄŸÄ±daki girdiyi analiz edin ve ana temalarÄ±n ve gÃ¶rÃ¼ÅŸlerin bir Ã¶zetini sunun. TÃ¼rkÃ§e cevap verin.","ko": "ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì œê³µí•œ ë‹¤ìŒ ìž…ë ¥ì„ ë¶„ì„í•˜ê³  ì£¼ìš” ì£¼ì œ ë° ì˜ê²¬ì— ëŒ€í•œ ìš”ì•½ì„ ì œê³µí•˜ì‹­ì‹œì˜¤. í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.","th": "à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰à¸—à¸µà¹ˆà¸Šà¸¸à¸¡à¸Šà¸™à¹ƒà¸«à¹‰à¸¡à¸²à¹à¸¥à¸°à¸ªà¸£à¸¸à¸›à¸›à¸£à¸°à¹€à¸”à¹‡à¸™à¸«à¸¥à¸±à¸à¹à¸¥à¸°à¸„à¸§à¸²à¸¡à¸„à¸´à¸”à¹€à¸«à¹‡à¸™ à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢",}

    system_message = ANALYSIS_SYSTEM_MSG.get(taal_code, ANALYSIS_SYSTEM_MSG["en"])
    user_prompt_instruction = ANALYSIS_USER_PROMPT.get(taal_code, ANALYSIS_USER_PROMPT["en"])

    geschiedenis = laad_community_input() # Assuming this loads from community_input_collection
    if not geschiedenis:
        logger.warning("perform_community_analysis: No input found to analyze.")
        return # Nothing to do

    gecombineerde_input = " ".join([entry["input"] for entry in geschiedenis])
    prompt = f"""
{user_prompt_instruction}

Community Input Data:
---
{gecombineerde_input}
---

Final Instruction: Ensure the entire summary response is ONLY in the requested language ({user_prompt_instruction.split()[-1].replace('.', '')}).
"""
    logger.debug(f"perform_community_analysis: Sending prompt to OpenAI:\nSystem: {system_message}\nUser: {prompt}")

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ]
        )
        ai_feedback = response["choices"][0]["message"]["content"]
        logger.info("perform_community_analysis: OpenAI analysis successful.")

        # --- Store Analysis Result ---
        analysis_result = {
            "ai_feedback": ai_feedback,
            "analysis_timestamp": datetime.utcnow(),
            "language_code": taal_code,
            "input_count_at_analysis": len(geschiedenis) # Store how many inputs were analyzed
        }
        # Update a single document (e.g., with a fixed ID) or insert new ones.
        # Using update_one with upsert=True is simple for storing only the latest.
        community_analysis_collection.update_one(
            {"_id": "latest_analysis"}, # Fixed ID for the latest analysis document
            {"$set": analysis_result},
            upsert=True
        )
        logger.info("perform_community_analysis: Stored analysis result in DB.")
        # --- End Store Analysis Result ---


        # --- Trigger Podcast Generation ---
        logger.info("Triggering podcast generation with analysis text.")
        try:
            podcast_generated = generate_podcast_audio(ai_feedback)
            if not podcast_generated:
                 logger.warning("Podcast generation function returned False.")
        except Exception as podcast_err:
            logger.error(f"Error calling generate_podcast_audio: {podcast_err}", exc_info=True)
        # --- End Podcast Trigger ---

    except openai.error.OpenAIError as e:
        logger.error(f"Error during OpenAI call in perform_community_analysis: {e}", exc_info=True)
    except Exception as e:
         logger.error(f"Unexpected error in perform_community_analysis: {e}", exc_info=True)

# --- End Helper Function ---

# --- ElevenLabs TTS Function ---
ELEVENLABS_VOICE_ID = "21m00Tcm4TlvDq8ikWAM" # Same voice as frontend
PODCAST_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'static', 'podcasts') # Assumes static folder exists at root level alongside blueprints

def generate_podcast_audio(text_to_speak):
    """Generates audio using ElevenLabs API and saves it."""
    api_key = os.getenv("ELEVENLABS_API_KEY")
    if not api_key:
        logger.error("ElevenLabs API key not found in environment variables.")
        return False

    tts_url = f"https://api.elevenlabs.io/v1/text-to-speech/{ELEVENLABS_VOICE_ID}"
    headers = {
        "Accept": "audio/mpeg",
        "Content-Type": "application/json",
        "xi-api-key": api_key
    }
    data = {
        "text": text_to_speak,
        "model_id": "eleven_monolingual_v1",
        "voice_settings": {
            "stability": 0.5,
            "similarity_boost": 0.75
        }
    }

    try:
        response = requests.post(tts_url, json=data, headers=headers, timeout=60) # Added timeout
        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

        # Ensure the podcast directory exists
        os.makedirs(PODCAST_DIR, exist_ok=True)

        # Create a unique filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"podcast_{timestamp}.mp3"
        filepath = os.path.join(PODCAST_DIR, filename)

        # Save the audio content
        with open(filepath, 'wb') as f:
            f.write(response.content)
        logger.info(f"Successfully generated and saved podcast: {filepath}")
        return True

    except requests.exceptions.RequestException as e:
        logger.error(f"Error calling ElevenLabs API: {e}", exc_info=True)
        # Log response body if available for more details on API errors
        if hasattr(e, 'response') and e.response is not None:
             logger.error(f"ElevenLabs API Response: {e.response.text}")
        return False
    except IOError as e:
        logger.error(f"Error saving podcast audio file: {e}", exc_info=True)
        return False
    except Exception as e:
        logger.error(f"An unexpected error occurred during podcast generation: {e}", exc_info=True)
        return False
# --- End ElevenLabs TTS Function ---

@community_bp.route("/community_input/analyse", methods=["GET"])
def analyseer_community_input():
    """Fetches the latest stored community analysis result."""
    logger.debug("Received request for /community_input/analyse (fetching latest stored result)")
    try:
        latest_analysis = community_analysis_collection.find_one({"_id": "latest_analysis"})
        if latest_analysis and "ai_feedback" in latest_analysis:
            logger.info("Found latest analysis result.")
            return jsonify({
                "status": "success",
                "ai_feedback": latest_analysis["ai_feedback"],
                "analysis_timestamp": latest_analysis.get("analysis_timestamp") # Optionally return timestamp
            })
        else:
            logger.warning("No stored analysis result found.")
            # Return a specific message indicating no analysis is available yet
            # Use user's language for the message if possible
            taal_code = get_country_language(session.get("gebruiker", {}).get("land", "other"))
            no_analysis_msg = {
                "en": "No community analysis has been generated yet. Submit more input!",
                "nl": "Er is nog geen community-analyse gegenereerd. Stuur meer input in!",
                # Add other languages as needed
            }
            message = no_analysis_msg.get(taal_code, no_analysis_msg["en"])
            return jsonify({"status": "success", "ai_feedback": message}) # Return success but with info message

    except Exception as e:
        logger.error(f"Error fetching latest community analysis: {e}", exc_info=True)
        return jsonify({"status": "error", "message": "Error retrieving analysis result."}), 500

@community_bp.route("/community_input/statistieken", methods=["GET"])
def community_statistieken():
    geschiedenis = laad_community_input()
    if not geschiedenis:
        return jsonify({"status": "error", "message": "Geen input beschikbaar voor statistieken."})

    alle_woorden = []
    bijdragers = Counter()

    for entry in geschiedenis:
        woorden = re.findall(r'\b\w+\b', entry["input"].lower())
        alle_woorden.extend(woorden)
        bijdragers[entry["gebruiker"]] += 1

    veelgebruikte_woorden = Counter(alle_woorden)
    populaire_themas = [woord for woord, count in veelgebruikte_woorden.most_common(5) if len(woord) > 3]
    top_bijdragers = [{"naam": naam, "aantal": aantal} for naam, aantal in bijdragers.most_common(5)]

    return jsonify({
        "status": "success",
        "populaire_themas": populaire_themas,
        "top_bijdragers": top_bijdragers
    })

@community_bp.route("/community_input/stats_by_country", methods=["GET"])
def community_stats_by_country():
    """Calculates and returns the count of community inputs per country using aggregation."""
    logger.debug("Received request for /community_input/stats_by_country")
    try:
        # Use aggregation pipeline to group by country and count
        pipeline = [
            # Removed $match stage to include documents without a country field
            {
                "$group": {
                    "_id": "$country",  # Group by the country field
                    "count": { "$sum": 1 } # Count documents in each group
                }
            },
            {
                "$project": {
                    "_id": 0, # Exclude the default _id field from output
                    "country": "$_id", # Rename _id (which is the country) to 'country'
                    "count": 1 # Include the count
                }
            },
            {
                 "$sort": { "count": -1 } # Optional: Sort by count descending
            }
        ]

        results = list(community_input_collection.aggregate(pipeline))

        # Convert list of {country: 'XX', count: N} to the desired { 'XX': N } format
        result_dict = {item['country']: item['count'] for item in results}

        logger.debug(f"Calculated country counts: {result_dict}")

        return jsonify({"status": "success", "country_counts": result_dict})

    except Exception as e:
        logger.error(f"Error calculating community stats by country: {e}", exc_info=True)
        return jsonify({"status": "error", "message": "Failed to calculate country statistics."}), 500
